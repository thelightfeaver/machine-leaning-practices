{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the required libraries\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "%pip install request\n",
    "%pip install matplotlib\n",
    "%pip install numpy\n",
    "%pip install joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the csv file\n",
    "df = pd.read_csv('../datasets/weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# Show quantity of each weather\n",
    "print(df['Weather'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show a resumen of dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete the column 'Date/Time'\n",
    "df = df.drop(columns=['Date/Time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename the columns\n",
    "df = df.rename(columns={'Temp_C': 'Temp', \n",
    "                        'Dew Point Temp_C': 'DewPoint', \n",
    "                        'Rel Hum_%': 'Humidity', \n",
    "                        'Wind Speed_km/h': 'WindSpeed', \n",
    "                        'Visibility_km': 'Visibility', \n",
    "                        'Press_kPa': 'Pressure'})\n",
    "\n",
    "# show columns\n",
    "df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Get the max and min of each column\n",
    "ls = ['Temp', 'DewPoint', 'Humidity', 'WindSpeed', 'Visibility', 'Pressure']\n",
    "data = {}\n",
    "\n",
    "for i in ls:\n",
    "    max = df[i].max()\n",
    "    min = df[i].min()\n",
    "\n",
    "    data[i] = {\n",
    "        'max': max,\n",
    "        'min': min\n",
    "    }\n",
    "\n",
    "pprint(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign the values of the variables X and Y\n",
    "X = df.drop('Weather', axis=1).values\n",
    "\n",
    "Y = df['Weather'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into training and testing sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Create a model and train it with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create a pipeline\n",
    "p = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                  StandardScaler(),\n",
    "                  MinMaxScaler(), \n",
    "                  LogisticRegression(max_iter=1000, n_jobs=3, verbose=0, solver='lbfgs'))\n",
    "\n",
    "# Train the model\n",
    "p.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "Y_pred = p.predict([X_test[0]])\n",
    "\n",
    "\n",
    "# Show the prediction\n",
    "print(Y_pred, Y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check accuracy of model\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix,f1_score\n",
    "\n",
    "print(\"Accuracy Score\",accuracy_score(Y_test, p.predict(X_test)))\n",
    "\n",
    "print(\"Confusion Matrix: \",confusion_matrix(Y_test, p.predict(X_test)))\n",
    "\n",
    "print(\"F1 Score:\", f1_score(Y_test, p.predict(X_test), average='weighted'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "\n",
    "# Create a pipeline\n",
    "p = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                  StandardScaler(),\n",
    "                  MinMaxScaler(), \n",
    "                  LogisticRegression(max_iter=1000, n_jobs=3, verbose=0, solver='lbfgs'))\n",
    "\n",
    "# Create a dictionary with the hyperparameters\n",
    "\n",
    "param_grid = {\n",
    "    'logisticregression__C': [0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['l1', 'l2'],\n",
    "    'logisticregression__solver': ['liblinear', 'lbfgs']\n",
    "}\n",
    "\n",
    "# Create a grid search\n",
    "\n",
    "cv = GridSearchCV(p, param_grid, n_jobs=3, verbose=0, cv=5)\n",
    "cv.fit(X_train, Y_train)\n",
    "\n",
    "# Show the best hyperparameters\n",
    "pprint(cv.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the best model\n",
    "model = LogisticRegression(C=10, penalty='l2', solver='lbfgs', max_iter=1000, n_jobs=3, verbose=0)\n",
    "p = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                  StandardScaler(),\n",
    "                  MinMaxScaler(), \n",
    "                  model)\n",
    "\n",
    "p.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the model\n",
    "import joblib\n",
    "\n",
    "joblib.dump(p, 'weather.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
